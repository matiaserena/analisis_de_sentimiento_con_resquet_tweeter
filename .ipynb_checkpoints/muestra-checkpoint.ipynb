{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T23:23:53.640457Z",
     "start_time": "2021-11-12T23:23:52.833363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk import SnowballStemmer          # para stemmizar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer # para tokenizar\n",
    "from nltk.corpus import stopwords         # para sacar las StopWords\n",
    "from nltk.stem import WordNetLemmatizer   # para lemmatizar \n",
    "from nltk.tokenize import RegexpTokenizer # para tokenizar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.warn('ignore')\n",
    "import kerastuner as kt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers  \n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T23:23:55.360329Z",
     "start_time": "2021-11-12T23:23:55.275696Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clasificación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es de las conspiranoicas,de las que creen que ...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>si elon musk saca un chip neuronal ME LO VOY A...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>elon Musk t odio m debes 5 de hachís</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lo siento twitter era un chiste aunque nose si...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tras lo sucedido con la IA de Google, nuevamen...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>los creadores en twitter deberían tener un pla...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>rt nicolás del caño elon musk es un parásito q...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>rt elon musk se enfrenta a una demanda de us 2...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>yaningnoviecita deskansa en paz elon musk no t...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>elon musk dijo q en tuiter iba a ver libertad ...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet Clasificación\n",
       "4     Es de las conspiranoicas,de las que creen que ...      Positivo\n",
       "5     si elon musk saca un chip neuronal ME LO VOY A...      Positivo\n",
       "6                  elon Musk t odio m debes 5 de hachís      Negativo\n",
       "8     lo siento twitter era un chiste aunque nose si...      Negativo\n",
       "9     Tras lo sucedido con la IA de Google, nuevamen...      Positivo\n",
       "...                                                 ...           ...\n",
       "1858  los creadores en twitter deberían tener un pla...      Negativo\n",
       "1865  rt nicolás del caño elon musk es un parásito q...      Negativo\n",
       "1867  rt elon musk se enfrenta a una demanda de us 2...      Negativo\n",
       "1868  yaningnoviecita deskansa en paz elon musk no t...      Negativo\n",
       "1873  elon musk dijo q en tuiter iba a ver libertad ...      Negativo\n",
       "\n",
       "[872 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('Tweets.csv',encoding='ANSI')\n",
    "df = df.drop(['Id','Fecha','Usuario'],axis=1)\n",
    "df = df[df.Clasificación != 'Neutro']\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweet Clasificación\n",
      "4     Es de las conspiranoicas,de las que creen que ...      Positivo\n",
      "5     si elon musk saca un chip neuronal ME LO VOY A...      Positivo\n",
      "6                  elon Musk t odio m debes 5 de hachís      Negativo\n",
      "8     lo siento twitter era un chiste aunque nose si...      Negativo\n",
      "9     Tras lo sucedido con la IA de Google, nuevamen...      Positivo\n",
      "...                                                 ...           ...\n",
      "1858  los creadores en twitter deberían tener un pla...      Negativo\n",
      "1865  rt nicolás del caño elon musk es un parásito q...      Negativo\n",
      "1867  rt elon musk se enfrenta a una demanda de us 2...      Negativo\n",
      "1868  yaningnoviecita deskansa en paz elon musk no t...      Negativo\n",
      "1873  elon musk dijo q en tuiter iba a ver libertad ...      Negativo\n",
      "\n",
      "[872 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def limpiar_token(texto):\n",
    "    # Se convierte todo el texto a minúsculas\n",
    "    nuevo_texto = texto.lower()\n",
    "    #sacar link\n",
    "    nuevo_texto = re.sub(f'\\S*[://]\\S*','', nuevo_texto)\n",
    "    #sacar @\n",
    "    \n",
    "    nuevo_texto = re.sub(f'@\\S+','', nuevo_texto)\n",
    "    nuevo_texto = re.sub(f'rt\\s@\\S+','', nuevo_texto)\n",
    "    # sacar #\n",
    "    #nuevo_texto = re.sub(f'#\\S+','', nuevo_texto)\n",
    "    #a,b = 'áéíóúüñ','aeiouun'\n",
    "    #trans = str.maketrans(a,b)\n",
    "    \n",
    "    #nuevo_texto = nuevo_texto.translate(trans)\n",
    "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('S+', ' ', nuevo_texto)\n",
    "    \n",
    "    #elimina numero\n",
    "    nuevo_texto = re.sub(f'[0-9]', ' ', nuevo_texto)\n",
    "    # Eliminación de signos de puntuación\n",
    "    regex = '[\\\\“\\\\!\\\\\"\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminación de espacios en blanco múltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "\n",
    "        \n",
    "    return nuevo_texto\n",
    "\n",
    "def tokenizador(txt):\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    txt_Token_pal = tokenizer.tokenize(txt)\n",
    "    return txt_Token_pal\n",
    "\n",
    "def not_stopwords(token_limpio):\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    notelon = ['elon','musk']\n",
    "    post_punctuation = [w for w in token_limpio if not w in stop_words]\n",
    "    post_punctuation = [w for w in post_punctuation if not w in notelon]\n",
    "    return post_punctuation\n",
    "\n",
    "\n",
    "def lemma_stemmer(word):\n",
    "    sp_Stemmer = SnowballStemmer('spanish')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lematizado = [lemmatizer.lemmatize(word) for word in word]\n",
    "    tok_lemma_stemmer = [sp_Stemmer.stem(lematizado) for lematizado in lematizado]\n",
    "    return tok_lemma_stemmer\n",
    "\n",
    "def cleanlist(cleamstr):\n",
    "    cleamstr = re.sub(f'[\\[\\]]','', cleamstr)\n",
    "    cleamstr = re.sub(f',','', cleamstr)\n",
    "    cleamstr = re.sub(f\"'\",'', cleamstr)\n",
    "    return cleamstr\n",
    "\n",
    "print(df)\n",
    "df['Tweet'] = df['Tweet'].transform(limpiar_token)\n",
    "df['Tweet'] = df['Tweet'].transform(tokenizador)\n",
    "df['Tweet'] = df['Tweet'].transform(not_stopwords)\n",
    "df['Tweet'] = df['Tweet'].transform(lemma_stemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clasificación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[conspiran, cre, niev, plastic, agu, hidrat, v...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[si, sac, chip, neuronal, voy, pon, cans]</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[t, odi, m, deb, hach]</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sient, twitt, chist, aunqu, nos, si, molest, ...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[tras, suced, ia, googl, nuev, hac, ruid, adve...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>[creador, twitt, deb, ten, plan, respald, si, ...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>[rt, nicolas, cañ, parasit, ademas, exig, empl...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>[rt, enfrent, demand, u, millon, acus, llev, c...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>[yaningnoviecit, deskans, paz, merec, bv]</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>[dij, q, tuit, iba, ver, libert, d, expresion,...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet Clasificación\n",
       "4     [conspiran, cre, niev, plastic, agu, hidrat, v...      Positivo\n",
       "5             [si, sac, chip, neuronal, voy, pon, cans]      Positivo\n",
       "6                                [t, odi, m, deb, hach]      Negativo\n",
       "8     [sient, twitt, chist, aunqu, nos, si, molest, ...      Negativo\n",
       "9     [tras, suced, ia, googl, nuev, hac, ruid, adve...      Positivo\n",
       "...                                                 ...           ...\n",
       "1858  [creador, twitt, deb, ten, plan, respald, si, ...      Negativo\n",
       "1865  [rt, nicolas, cañ, parasit, ademas, exig, empl...      Negativo\n",
       "1867  [rt, enfrent, demand, u, millon, acus, llev, c...      Negativo\n",
       "1868          [yaningnoviecit, deskans, paz, merec, bv]      Negativo\n",
       "1873  [dij, q, tuit, iba, ver, libert, d, expresion,...      Negativo\n",
       "\n",
       "[872 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       ['conspiran', 'cre', 'niev', 'plastic', 'agu',...\n",
       "5       ['si', 'sac', 'chip', 'neuronal', 'voy', 'pon'...\n",
       "6                        ['t', 'odi', 'm', 'deb', 'hach']\n",
       "8       ['sient', 'twitt', 'chist', 'aunqu', 'nos', 's...\n",
       "9       ['tras', 'suced', 'ia', 'googl', 'nuev', 'hac'...\n",
       "                              ...                        \n",
       "1858    ['creador', 'twitt', 'deb', 'ten', 'plan', 're...\n",
       "1865    ['rt', 'nicolas', 'cañ', 'parasit', 'ademas', ...\n",
       "1867    ['rt', 'enfrent', 'demand', 'u', 'millon', 'ac...\n",
       "1868    ['yaningnoviecit', 'deskans', 'paz', 'merec', ...\n",
       "1873    ['dij', 'q', 'tuit', 'iba', 'ver', 'libert', '...\n",
       "Name: Tweet, Length: 872, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(lambda _: str(_))\n",
    "df['Tweet']\n",
    "#df = df.groupby(by=['Clasificación'],as_index=False)['Tweet'].apply(' '.join).reset_index(drop = True)\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abaj</th>\n",
       "      <th>abarc</th>\n",
       "      <th>abc</th>\n",
       "      <th>abiert</th>\n",
       "      <th>abog</th>\n",
       "      <th>abort</th>\n",
       "      <th>abra</th>\n",
       "      <th>abre</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youtubers</th>\n",
       "      <th>zanj</th>\n",
       "      <th>zar</th>\n",
       "      <th>zelenski</th>\n",
       "      <th>zocal</th>\n",
       "      <th>zonasrural</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zurd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows × 2807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aaa  aaron  abaj  abarc  abc  abiert  abog  abort  abra  abre  ...  you  \\\n",
       "0      0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "1      0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "2      0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "3      0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "4      0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "..   ...    ...   ...    ...  ...     ...   ...    ...   ...   ...  ...  ...   \n",
       "867    0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "868    0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "869    0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "870    0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "871    0      0     0      0    0       0     0      0     0     0  ...    0   \n",
       "\n",
       "     young  youtubers  zanj  zar  zelenski  zocal  zonasrural  zuckerberg  \\\n",
       "0        0          0     0    0         0      0           0           0   \n",
       "1        0          0     0    0         0      0           0           0   \n",
       "2        0          0     0    0         0      0           0           0   \n",
       "3        0          0     0    0         0      0           0           0   \n",
       "4        0          0     0    0         0      0           0           0   \n",
       "..     ...        ...   ...  ...       ...    ...         ...         ...   \n",
       "867      0          0     0    0         0      0           0           0   \n",
       "868      0          0     0    0         0      0           0           0   \n",
       "869      0          0     0    0         0      0           0           0   \n",
       "870      0          0     0    0         0      0           0           0   \n",
       "871      0          0     0    0         0      0           0           0   \n",
       "\n",
       "     zurd  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "..    ...  \n",
       "867     0  \n",
       "868     0  \n",
       "869     0  \n",
       "870     0  \n",
       "871     0  \n",
       "\n",
       "[872 rows x 2807 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Tweet'])\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "X=pd.DataFrame(X.toarray())\n",
    "\n",
    "X.columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(df['Clasificación'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca_auto=PCA(n_components = 100)\n",
    "X= pd.DataFrame(pca_auto.fit_transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.replace({'Positivo':1,'Negativo':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.groupby(['Clasificación'])['Clasificación'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "modelo = Sequential()\n",
    "\n",
    "modelo.add(Input(shape=(100,), name = \"Entrada\"))\n",
    "    \n",
    "modelo.add( keras.layers.Dense(units = 20, kernel_initializer = 'he_normal', activation= 'elu' ))\n",
    "\n",
    "modelo.add( keras.layers.Dense(units = 4 , kernel_initializer = 'he_normal', activation= 'elu' ))\n",
    " \n",
    "modelo.add( keras.layers.Dense(units = 41, kernel_initializer = 'he_normal', activation= 'elu' ))    \n",
    "\n",
    "modelo.add( keras.layers.Dense(units = 1 , kernel_initializer = 'he_normal' , name=\"Salida\",activation= 'sigmoid')) \n",
    "    \n",
    "\n",
    "modelo.compile(loss='binary_crossentropy',optimizer = 'Nadam',metrics=['accuracy'])\n",
    "\n",
    "modelo.fit(X, y, epochs=100 ,batch_size=32)\n",
    "df_history=pd.DataFrame(modelo.history.history)\n",
    "print(df_history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df_history.plot(figsize=(12, 8),grid=True, title=\"Learning Curves\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#pred['Tweet'] = input('Escribe un tweet sobre Elon Musk: ')\n",
    "pred = pd.DataFrame([input('Escribe...')])\n",
    "pred.columns = [0]\n",
    "pred[0] = pred[0].transform(limpiar_token)\n",
    "pred[0] = pred[0].transform(tokenizador)\n",
    "pred[0] = pred[0].transform(not_stopwords)\n",
    "pred[0] = pred[0].transform(lemma_stemmer)\n",
    "pred[0] = pred[0].apply(lambda _: str(_))\n",
    "\n",
    "\n",
    "X2 = vectorizer.transform(pred[0])\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "X2=pd.DataFrame(X2.toarray())\n",
    "X2.columns = vectorizer.get_feature_names_out()\n",
    "X2 = pca_auto.transform(X2)\n",
    "pripredicion = modelo.predict(X2)\n",
    "\n",
    "print(pripredicion)\n",
    "if pripredicion>0.50:\n",
    "    print('🟢😁🟢')\n",
    "elif pripredicion<0.50:\n",
    "    print('🔴😡🔴')\n",
    "else:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no son tan bueno los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
